<p><strong>Slurm</strong> is a workload manager for distributed systems.</p>

<h2 id="the-most-important-commands">The most important commands</h2>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>squeue -u USERNAME
squeue -p PARTITION
sbatch SCRIPT
scancel SLURM_ID
</code></pre></div></div>

<p>squeue -u USERNAME will tell you what jobs are running for the user. You can keep track of your jobs this way, although it only tells you which ones are running. You can look at what is happening (to a degree) by checking the log files which appear in your home directory (or somewhere else if you specify it). If you’re running a python script and want to monitor what’s happening, it’s better to use <a href="https://docs.python.org/2/howto/logging-cookbook.html">logging</a> rather than print statements, as the print statements only appear in the log file after the whole script is finished running.</p>

<p>squeue -p PARTITION will show you all the jobs being run on a specific partition. In the case of Saga, we usually stick to normal and accel. Accel has GPUs, so if you’re running a compute heavy job, it’s probably better.</p>

<p>sbatch SCRIPT will run a slurm job that you have set up. This also give you the SLURM_ID for the job. If you need to cancel the job for some reason, run scancel SLURM_ID</p>

<h2 id="importing--setting-up-necessary-libraries-and-dependencies">Importing / Setting up necessary libraries and dependencies</h2>

<p>Saga already has most of the dependencies you need installed (Pytorch, Sklearn, etc). You can check available modules <a href="http://wiki.nlpl.eu/index.php/Infrastructure/software/catalogue">here</a>.</p>

<p>To upload a module, either in an interactive session (when you first log on), do:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>module load MODULENAME
</code></pre></div></div>

<h2 id="example-slurm-job">Example slurm job</h2>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>
<span class="c">#</span>
<span class="c">#SBATCH --job-name=bert_finetune --account=nn9447k  #change the job-name</span>
<span class="c">#SBATCH --output=bert_finetune-%j.out               #name of log file</span>
<span class="c">#</span>
<span class="c">#SBATCH --partition=normal                          #normal or accel</span>
<span class="c">#SBATCH --time=48:00:00                             #how long to run</span>
<span class="c">#</span>
<span class="c">#</span>
<span class="c"># Max memory usage:</span>
<span class="c">#SBATCH --mem-per-cpu=100G</span>
<span class="c">#</span>
<span class="c"># Number of threads per task:</span>
<span class="c">#SBATCH --cpus-per-task=1</span>

<span class="c">## Set up job environtment:</span>
<span class="c"># source /cluster/bin/jobsetup</span>
module restore system                              <span class="c">#clear any inherited modules</span>
<span class="nb">set</span> <span class="nt">-o</span> errexit                                     <span class="c">#exit on errors</span>

<span class="c"># List imported modules</span>
module list

<span class="c"># Load models</span>
module load Python/3.6.6-fosscuda-2018b

<span class="c">## In addition to the models already available, sometimes I need extra stuff.</span>
<span class="c">## I set up virtual environments and use pip --user install X to get what I</span>
<span class="c">## need.</span>

<span class="c"># load virtual environment</span>
<span class="nb">source</span> /cluster/home/jeremycb/doc_level_transfer_hierarchical/my_virtualenv/bin/activate


<span class="c"># Run the python script</span>

python task_tuning.py <span class="nt">--data_dir</span><span class="o">=</span><span class="s2">"./data"</span> <span class="nt">--bert_model</span><span class="o">=</span><span class="s2">"bert-base-cased"</span> <span class="nt">--output_dir</span><span class="o">=</span><span class="s2">"task_tuned/ontonotes-reuters-50000"</span> <span class="nt">--trained_model_dir</span><span class="o">=</span><span class="s2">"lm_output/ontonotes-reuters"</span> <span class="nt">--do_train</span> <span class="nt">--do_eval</span> <span class="nt">--save_all_epochs</span> <span class="nt">--num_training_examples</span><span class="o">=</span>50000

</code></pre></div></div>

